{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing lib"
      ],
      "metadata": {
        "id": "9l57rAkjSMXA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r1z05BmCtBya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4b1591-6b41-4ec0-cdb9-2b16537350d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "corpus = \"\"\"Hello, Welcome to Malla Rddy University. NLP tutoyials!!!\"\"\""
      ],
      "metadata": {
        "id": "ORsuaubYTvof"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jwt7DmlbVm_m",
        "outputId": "debd7bd3-2a12-423c-dad2-d3dbe3e258fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, Welcome to Malla Rddy University. NLP tutoyials!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "d9HtPxCOVqSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeRyJexVVoVT",
        "outputId": "d8709955-65ed-430b-8cf9-7b5d33ff4936"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "lKAXWg_OWApt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIdsgwUcWnC5",
        "outputId": "62782543-74d0-4eba-df54-76966d3eae1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello, Welcome to Malla Rddy University.', 'NLP tutoyials!!', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "docs = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "7YOBPfFlXH6O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQiSbLKiXOrj",
        "outputId": "cbd3fa1d-6818-455e-c5f7-ee3389706809"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'Malla',\n",
              " 'Rddy',\n",
              " 'University',\n",
              " '.',\n",
              " 'NLP',\n",
              " 'tutoyials',\n",
              " '!',\n",
              " '!',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Reduces the words to its root form though it does not hav any meaning."
      ],
      "metadata": {
        "id": "wNHWAUUbXTNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\",\"history\",\"eaten\",\"finally\"]"
      ],
      "metadata": {
        "id": "jPyn3y2oXnfF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "fc00jnuzXPfs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "pfdZTWVxXenx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word + \"---->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbZRwFd3XhzD",
        "outputId": "74c91609-9261-48b0-c19e-24197933398c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program---->program\n",
            "programs---->program\n",
            "programer---->program\n",
            "programing---->program\n",
            "programers---->program\n",
            "history---->histori\n",
            "eaten---->eaten\n",
            "finally---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yHJTqs1qXq0W",
        "outputId": "c4ad5bdc-c73d-463e-ad62-618d6e8eb516"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('sitting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Am-agrIYYhda",
        "outputId": "2d1be566-4b86-4963-d72f-f1a4b41f2d32"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('Engineering')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JNzTAgAxYnGq",
        "outputId": "899e2df5-c03a-4cdd-b555-edac8b9cd8a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'engin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegexpStemmer class"
      ],
      "metadata": {
        "id": "1j1ZOQYjY_mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "mw2uoE7iY86n"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "55KMVK_2ZK0I"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-WztExFmZQkt",
        "outputId": "e48a5ff8-9354-4845-e4a9-6f65b4c87621"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3Spbhn6qZTRg",
        "outputId": "54154b4c-d0ca-4f44-f014-852d1df8b909"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Snowball Stemmer"
      ],
      "metadata": {
        "id": "jr5PnVTFZw9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "ZrIIMlUHZcO5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "RDL7BJ-0aBlb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word + \"---->\"+snowballsstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZtlQuzNaG7g",
        "outputId": "cec6ded4-6d4b-463a-e428-e73e16e9cb8b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program---->program\n",
            "programs---->program\n",
            "programer---->program\n",
            "programing---->program\n",
            "programers---->program\n",
            "history---->histori\n",
            "eaten---->eaten\n",
            "finally---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"fairly\"), stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILhB3NhEaVJ3",
        "outputId": "b845f0c5-dbb5-4fa2-f729-241907c1f6cf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer.stem(\"fairly\"), snowballsstemmer.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQI4fZ1zadHr",
        "outputId": "bc3c811b-6b25-4bb3-9020-0b511cbde080"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer.stem(\"goes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tIgZPvfoahn5",
        "outputId": "c2527bd1-4897-420b-8366-1e50d2e979f2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"goes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qTbsuGL2an6P",
        "outputId": "adc340b9-8372-4f74-800e-86cfab02afc8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "SP1yK8TPbZWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "C8ASM8UFats_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "2l5zCT-Ubujq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\", pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L_N2Qdx5bypg",
        "outputId": "e967c53b-c1ce-42d8-99a7-db4aa6973093"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\",\"history\",\"eaten\",\"finally\"]"
      ],
      "metadata": {
        "id": "Kgg9Jre-cAwv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word + \"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edJBiFvNcH4W",
        "outputId": "c6d4d96a-35b0-48d4-cb19-18d9581e17ac"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program---->program\n",
            "programs---->program\n",
            "programer---->programer\n",
            "programing---->program\n",
            "programers---->programers\n",
            "history---->history\n",
            "eaten---->eat\n",
            "finally---->finally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word + \"---->\"+lemmatizer.lemmatize(word,pos='a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2FBhDUzcNEk",
        "outputId": "16d07c6c-396b-402d-bb70-0c1dae41530b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program---->program\n",
            "programs---->programs\n",
            "programer---->programer\n",
            "programing---->programing\n",
            "programers---->programers\n",
            "history---->history\n",
            "eaten---->eaten\n",
            "finally---->finally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word + \"---->\"+lemmatizer.lemmatize(word,pos='n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ6itviTcjTW",
        "outputId": "83875e84-4d96-4d22-d0a7-05caa35c9de2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program---->program\n",
            "programs---->program\n",
            "programer---->programer\n",
            "programing---->programing\n",
            "programers---->programers\n",
            "history---->history\n",
            "eaten---->eaten\n",
            "finally---->finally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"fairly\",pos = 'v'), lemmatizer.lemmatize(\"sportingly\",pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axU5bcercqQC",
        "outputId": "1471a0c6-c128-4387-8dba-918d91944185"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ],
      "metadata": {
        "id": "gBNFlmnJc2kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph= \"\"\"Once, there lived a king. The king was very rich. The king served many people in the kingdom. He was the richest man in the kingdom.\"\"\""
      ],
      "metadata": {
        "id": "X8rJHQY2c01z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "iI_tGxfHdADM"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download ('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeOCTxz_dKJE",
        "outputId": "0a1e9f20-f3f0-4c4e-8d92-fa4e2bb2cdb8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpPiky4udOp4",
        "outputId": "4b9f063e-fbc4-451a-9d30-600ac20e4d4d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('arabic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P65edluwdUnw",
        "outputId": "2bf3212f-1de6-4477-f549-9967db68b886"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['إذ',\n",
              " 'إذا',\n",
              " 'إذما',\n",
              " 'إذن',\n",
              " 'أف',\n",
              " 'أقل',\n",
              " 'أكثر',\n",
              " 'ألا',\n",
              " 'إلا',\n",
              " 'التي',\n",
              " 'الذي',\n",
              " 'الذين',\n",
              " 'اللاتي',\n",
              " 'اللائي',\n",
              " 'اللتان',\n",
              " 'اللتيا',\n",
              " 'اللتين',\n",
              " 'اللذان',\n",
              " 'اللذين',\n",
              " 'اللواتي',\n",
              " 'إلى',\n",
              " 'إليك',\n",
              " 'إليكم',\n",
              " 'إليكما',\n",
              " 'إليكن',\n",
              " 'أم',\n",
              " 'أما',\n",
              " 'أما',\n",
              " 'إما',\n",
              " 'أن',\n",
              " 'إن',\n",
              " 'إنا',\n",
              " 'أنا',\n",
              " 'أنت',\n",
              " 'أنتم',\n",
              " 'أنتما',\n",
              " 'أنتن',\n",
              " 'إنما',\n",
              " 'إنه',\n",
              " 'أنى',\n",
              " 'أنى',\n",
              " 'آه',\n",
              " 'آها',\n",
              " 'أو',\n",
              " 'أولاء',\n",
              " 'أولئك',\n",
              " 'أوه',\n",
              " 'آي',\n",
              " 'أي',\n",
              " 'أيها',\n",
              " 'إي',\n",
              " 'أين',\n",
              " 'أين',\n",
              " 'أينما',\n",
              " 'إيه',\n",
              " 'بخ',\n",
              " 'بس',\n",
              " 'بعد',\n",
              " 'بعض',\n",
              " 'بك',\n",
              " 'بكم',\n",
              " 'بكم',\n",
              " 'بكما',\n",
              " 'بكن',\n",
              " 'بل',\n",
              " 'بلى',\n",
              " 'بما',\n",
              " 'بماذا',\n",
              " 'بمن',\n",
              " 'بنا',\n",
              " 'به',\n",
              " 'بها',\n",
              " 'بهم',\n",
              " 'بهما',\n",
              " 'بهن',\n",
              " 'بي',\n",
              " 'بين',\n",
              " 'بيد',\n",
              " 'تلك',\n",
              " 'تلكم',\n",
              " 'تلكما',\n",
              " 'ته',\n",
              " 'تي',\n",
              " 'تين',\n",
              " 'تينك',\n",
              " 'ثم',\n",
              " 'ثمة',\n",
              " 'حاشا',\n",
              " 'حبذا',\n",
              " 'حتى',\n",
              " 'حيث',\n",
              " 'حيثما',\n",
              " 'حين',\n",
              " 'خلا',\n",
              " 'دون',\n",
              " 'ذا',\n",
              " 'ذات',\n",
              " 'ذاك',\n",
              " 'ذان',\n",
              " 'ذانك',\n",
              " 'ذلك',\n",
              " 'ذلكم',\n",
              " 'ذلكما',\n",
              " 'ذلكن',\n",
              " 'ذه',\n",
              " 'ذو',\n",
              " 'ذوا',\n",
              " 'ذواتا',\n",
              " 'ذواتي',\n",
              " 'ذي',\n",
              " 'ذين',\n",
              " 'ذينك',\n",
              " 'ريث',\n",
              " 'سوف',\n",
              " 'سوى',\n",
              " 'شتان',\n",
              " 'عدا',\n",
              " 'عسى',\n",
              " 'عل',\n",
              " 'على',\n",
              " 'عليك',\n",
              " 'عليه',\n",
              " 'عما',\n",
              " 'عن',\n",
              " 'عند',\n",
              " 'غير',\n",
              " 'فإذا',\n",
              " 'فإن',\n",
              " 'فلا',\n",
              " 'فمن',\n",
              " 'في',\n",
              " 'فيم',\n",
              " 'فيما',\n",
              " 'فيه',\n",
              " 'فيها',\n",
              " 'قد',\n",
              " 'كأن',\n",
              " 'كأنما',\n",
              " 'كأي',\n",
              " 'كأين',\n",
              " 'كذا',\n",
              " 'كذلك',\n",
              " 'كل',\n",
              " 'كلا',\n",
              " 'كلاهما',\n",
              " 'كلتا',\n",
              " 'كلما',\n",
              " 'كليكما',\n",
              " 'كليهما',\n",
              " 'كم',\n",
              " 'كم',\n",
              " 'كما',\n",
              " 'كي',\n",
              " 'كيت',\n",
              " 'كيف',\n",
              " 'كيفما',\n",
              " 'لا',\n",
              " 'لاسيما',\n",
              " 'لدى',\n",
              " 'لست',\n",
              " 'لستم',\n",
              " 'لستما',\n",
              " 'لستن',\n",
              " 'لسن',\n",
              " 'لسنا',\n",
              " 'لعل',\n",
              " 'لك',\n",
              " 'لكم',\n",
              " 'لكما',\n",
              " 'لكن',\n",
              " 'لكنما',\n",
              " 'لكي',\n",
              " 'لكيلا',\n",
              " 'لم',\n",
              " 'لما',\n",
              " 'لن',\n",
              " 'لنا',\n",
              " 'له',\n",
              " 'لها',\n",
              " 'لهم',\n",
              " 'لهما',\n",
              " 'لهن',\n",
              " 'لو',\n",
              " 'لولا',\n",
              " 'لوما',\n",
              " 'لي',\n",
              " 'لئن',\n",
              " 'ليت',\n",
              " 'ليس',\n",
              " 'ليسا',\n",
              " 'ليست',\n",
              " 'ليستا',\n",
              " 'ليسوا',\n",
              " 'ما',\n",
              " 'ماذا',\n",
              " 'متى',\n",
              " 'مذ',\n",
              " 'مع',\n",
              " 'مما',\n",
              " 'ممن',\n",
              " 'من',\n",
              " 'منه',\n",
              " 'منها',\n",
              " 'منذ',\n",
              " 'مه',\n",
              " 'مهما',\n",
              " 'نحن',\n",
              " 'نحو',\n",
              " 'نعم',\n",
              " 'ها',\n",
              " 'هاتان',\n",
              " 'هاته',\n",
              " 'هاتي',\n",
              " 'هاتين',\n",
              " 'هاك',\n",
              " 'هاهنا',\n",
              " 'هذا',\n",
              " 'هذان',\n",
              " 'هذه',\n",
              " 'هذي',\n",
              " 'هذين',\n",
              " 'هكذا',\n",
              " 'هل',\n",
              " 'هلا',\n",
              " 'هم',\n",
              " 'هما',\n",
              " 'هن',\n",
              " 'هنا',\n",
              " 'هناك',\n",
              " 'هنالك',\n",
              " 'هو',\n",
              " 'هؤلاء',\n",
              " 'هي',\n",
              " 'هيا',\n",
              " 'هيت',\n",
              " 'هيهات',\n",
              " 'والذي',\n",
              " 'والذين',\n",
              " 'وإذ',\n",
              " 'وإذا',\n",
              " 'وإن',\n",
              " 'ولا',\n",
              " 'ولكن',\n",
              " 'ولو',\n",
              " 'وما',\n",
              " 'ومن',\n",
              " 'وهو',\n",
              " 'يا',\n",
              " 'أبٌ',\n",
              " 'أخٌ',\n",
              " 'حمٌ',\n",
              " 'فو',\n",
              " 'أنتِ',\n",
              " 'يناير',\n",
              " 'فبراير',\n",
              " 'مارس',\n",
              " 'أبريل',\n",
              " 'مايو',\n",
              " 'يونيو',\n",
              " 'يوليو',\n",
              " 'أغسطس',\n",
              " 'سبتمبر',\n",
              " 'أكتوبر',\n",
              " 'نوفمبر',\n",
              " 'ديسمبر',\n",
              " 'جانفي',\n",
              " 'فيفري',\n",
              " 'مارس',\n",
              " 'أفريل',\n",
              " 'ماي',\n",
              " 'جوان',\n",
              " 'جويلية',\n",
              " 'أوت',\n",
              " 'كانون',\n",
              " 'شباط',\n",
              " 'آذار',\n",
              " 'نيسان',\n",
              " 'أيار',\n",
              " 'حزيران',\n",
              " 'تموز',\n",
              " 'آب',\n",
              " 'أيلول',\n",
              " 'تشرين',\n",
              " 'دولار',\n",
              " 'دينار',\n",
              " 'ريال',\n",
              " 'درهم',\n",
              " 'ليرة',\n",
              " 'جنيه',\n",
              " 'قرش',\n",
              " 'مليم',\n",
              " 'فلس',\n",
              " 'هللة',\n",
              " 'سنتيم',\n",
              " 'يورو',\n",
              " 'ين',\n",
              " 'يوان',\n",
              " 'شيكل',\n",
              " 'واحد',\n",
              " 'اثنان',\n",
              " 'ثلاثة',\n",
              " 'أربعة',\n",
              " 'خمسة',\n",
              " 'ستة',\n",
              " 'سبعة',\n",
              " 'ثمانية',\n",
              " 'تسعة',\n",
              " 'عشرة',\n",
              " 'أحد',\n",
              " 'اثنا',\n",
              " 'اثني',\n",
              " 'إحدى',\n",
              " 'ثلاث',\n",
              " 'أربع',\n",
              " 'خمس',\n",
              " 'ست',\n",
              " 'سبع',\n",
              " 'ثماني',\n",
              " 'تسع',\n",
              " 'عشر',\n",
              " 'ثمان',\n",
              " 'سبت',\n",
              " 'أحد',\n",
              " 'اثنين',\n",
              " 'ثلاثاء',\n",
              " 'أربعاء',\n",
              " 'خميس',\n",
              " 'جمعة',\n",
              " 'أول',\n",
              " 'ثان',\n",
              " 'ثاني',\n",
              " 'ثالث',\n",
              " 'رابع',\n",
              " 'خامس',\n",
              " 'سادس',\n",
              " 'سابع',\n",
              " 'ثامن',\n",
              " 'تاسع',\n",
              " 'عاشر',\n",
              " 'حادي',\n",
              " 'أ',\n",
              " 'ب',\n",
              " 'ت',\n",
              " 'ث',\n",
              " 'ج',\n",
              " 'ح',\n",
              " 'خ',\n",
              " 'د',\n",
              " 'ذ',\n",
              " 'ر',\n",
              " 'ز',\n",
              " 'س',\n",
              " 'ش',\n",
              " 'ص',\n",
              " 'ض',\n",
              " 'ط',\n",
              " 'ظ',\n",
              " 'ع',\n",
              " 'غ',\n",
              " 'ف',\n",
              " 'ق',\n",
              " 'ك',\n",
              " 'ل',\n",
              " 'م',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ي',\n",
              " 'ء',\n",
              " 'ى',\n",
              " 'آ',\n",
              " 'ؤ',\n",
              " 'ئ',\n",
              " 'أ',\n",
              " 'ة',\n",
              " 'ألف',\n",
              " 'باء',\n",
              " 'تاء',\n",
              " 'ثاء',\n",
              " 'جيم',\n",
              " 'حاء',\n",
              " 'خاء',\n",
              " 'دال',\n",
              " 'ذال',\n",
              " 'راء',\n",
              " 'زاي',\n",
              " 'سين',\n",
              " 'شين',\n",
              " 'صاد',\n",
              " 'ضاد',\n",
              " 'طاء',\n",
              " 'ظاء',\n",
              " 'عين',\n",
              " 'غين',\n",
              " 'فاء',\n",
              " 'قاف',\n",
              " 'كاف',\n",
              " 'لام',\n",
              " 'ميم',\n",
              " 'نون',\n",
              " 'هاء',\n",
              " 'واو',\n",
              " 'ياء',\n",
              " 'همزة',\n",
              " 'ي',\n",
              " 'نا',\n",
              " 'ك',\n",
              " 'كن',\n",
              " 'ه',\n",
              " 'إياه',\n",
              " 'إياها',\n",
              " 'إياهما',\n",
              " 'إياهم',\n",
              " 'إياهن',\n",
              " 'إياك',\n",
              " 'إياكما',\n",
              " 'إياكم',\n",
              " 'إياك',\n",
              " 'إياكن',\n",
              " 'إياي',\n",
              " 'إيانا',\n",
              " 'أولالك',\n",
              " 'تانِ',\n",
              " 'تانِك',\n",
              " 'تِه',\n",
              " 'تِي',\n",
              " 'تَيْنِ',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'ذانِ',\n",
              " 'ذِه',\n",
              " 'ذِي',\n",
              " 'ذَيْنِ',\n",
              " 'هَؤلاء',\n",
              " 'هَاتانِ',\n",
              " 'هَاتِه',\n",
              " 'هَاتِي',\n",
              " 'هَاتَيْنِ',\n",
              " 'هَذا',\n",
              " 'هَذانِ',\n",
              " 'هَذِه',\n",
              " 'هَذِي',\n",
              " 'هَذَيْنِ',\n",
              " 'الألى',\n",
              " 'الألاء',\n",
              " 'أل',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'ذيت',\n",
              " 'كأيّ',\n",
              " 'كأيّن',\n",
              " 'بضع',\n",
              " 'فلان',\n",
              " 'وا',\n",
              " 'آمينَ',\n",
              " 'آهِ',\n",
              " 'آهٍ',\n",
              " 'آهاً',\n",
              " 'أُفٍّ',\n",
              " 'أُفٍّ',\n",
              " 'أفٍّ',\n",
              " 'أمامك',\n",
              " 'أمامكَ',\n",
              " 'أوّهْ',\n",
              " 'إلَيْكَ',\n",
              " 'إلَيْكَ',\n",
              " 'إليكَ',\n",
              " 'إليكنّ',\n",
              " 'إيهٍ',\n",
              " 'بخٍ',\n",
              " 'بسّ',\n",
              " 'بَسْ',\n",
              " 'بطآن',\n",
              " 'بَلْهَ',\n",
              " 'حاي',\n",
              " 'حَذارِ',\n",
              " 'حيَّ',\n",
              " 'حيَّ',\n",
              " 'دونك',\n",
              " 'رويدك',\n",
              " 'سرعان',\n",
              " 'شتانَ',\n",
              " 'شَتَّانَ',\n",
              " 'صهْ',\n",
              " 'صهٍ',\n",
              " 'طاق',\n",
              " 'طَق',\n",
              " 'عَدَسْ',\n",
              " 'كِخ',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانكم',\n",
              " 'مكانكما',\n",
              " 'مكانكنّ',\n",
              " 'نَخْ',\n",
              " 'هاكَ',\n",
              " 'هَجْ',\n",
              " 'هلم',\n",
              " 'هيّا',\n",
              " 'هَيْهات',\n",
              " 'وا',\n",
              " 'واهاً',\n",
              " 'وراءَك',\n",
              " 'وُشْكَانَ',\n",
              " 'وَيْ',\n",
              " 'يفعلان',\n",
              " 'تفعلان',\n",
              " 'يفعلون',\n",
              " 'تفعلون',\n",
              " 'تفعلين',\n",
              " 'اتخذ',\n",
              " 'ألفى',\n",
              " 'تخذ',\n",
              " 'ترك',\n",
              " 'تعلَّم',\n",
              " 'جعل',\n",
              " 'حجا',\n",
              " 'حبيب',\n",
              " 'خال',\n",
              " 'حسب',\n",
              " 'خال',\n",
              " 'درى',\n",
              " 'رأى',\n",
              " 'زعم',\n",
              " 'صبر',\n",
              " 'ظنَّ',\n",
              " 'عدَّ',\n",
              " 'علم',\n",
              " 'غادر',\n",
              " 'ذهب',\n",
              " 'وجد',\n",
              " 'ورد',\n",
              " 'وهب',\n",
              " 'أسكن',\n",
              " 'أطعم',\n",
              " 'أعطى',\n",
              " 'رزق',\n",
              " 'زود',\n",
              " 'سقى',\n",
              " 'كسا',\n",
              " 'أخبر',\n",
              " 'أرى',\n",
              " 'أعلم',\n",
              " 'أنبأ',\n",
              " 'حدَث',\n",
              " 'خبَّر',\n",
              " 'نبَّا',\n",
              " 'أفعل به',\n",
              " 'ما أفعله',\n",
              " 'بئس',\n",
              " 'ساء',\n",
              " 'طالما',\n",
              " 'قلما',\n",
              " 'لات',\n",
              " 'لكنَّ',\n",
              " 'ءَ',\n",
              " 'أجل',\n",
              " 'إذاً',\n",
              " 'أمّا',\n",
              " 'إمّا',\n",
              " 'إنَّ',\n",
              " 'أنًّ',\n",
              " 'أى',\n",
              " 'إى',\n",
              " 'أيا',\n",
              " 'ب',\n",
              " 'ثمَّ',\n",
              " 'جلل',\n",
              " 'جير',\n",
              " 'رُبَّ',\n",
              " 'س',\n",
              " 'علًّ',\n",
              " 'ف',\n",
              " 'كأنّ',\n",
              " 'كلَّا',\n",
              " 'كى',\n",
              " 'ل',\n",
              " 'لات',\n",
              " 'لعلَّ',\n",
              " 'لكنَّ',\n",
              " 'لكنَّ',\n",
              " 'م',\n",
              " 'نَّ',\n",
              " 'هلّا',\n",
              " 'وا',\n",
              " 'أل',\n",
              " 'إلّا',\n",
              " 'ت',\n",
              " 'ك',\n",
              " 'لمّا',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ا',\n",
              " 'ي',\n",
              " 'تجاه',\n",
              " 'تلقاء',\n",
              " 'جميع',\n",
              " 'حسب',\n",
              " 'سبحان',\n",
              " 'شبه',\n",
              " 'لعمر',\n",
              " 'مثل',\n",
              " 'معاذ',\n",
              " 'أبو',\n",
              " 'أخو',\n",
              " 'حمو',\n",
              " 'فو',\n",
              " 'مئة',\n",
              " 'مئتان',\n",
              " 'ثلاثمئة',\n",
              " 'أربعمئة',\n",
              " 'خمسمئة',\n",
              " 'ستمئة',\n",
              " 'سبعمئة',\n",
              " 'ثمنمئة',\n",
              " 'تسعمئة',\n",
              " 'مائة',\n",
              " 'ثلاثمائة',\n",
              " 'أربعمائة',\n",
              " 'خمسمائة',\n",
              " 'ستمائة',\n",
              " 'سبعمائة',\n",
              " 'ثمانمئة',\n",
              " 'تسعمائة',\n",
              " 'عشرون',\n",
              " 'ثلاثون',\n",
              " 'اربعون',\n",
              " 'خمسون',\n",
              " 'ستون',\n",
              " 'سبعون',\n",
              " 'ثمانون',\n",
              " 'تسعون',\n",
              " 'عشرين',\n",
              " 'ثلاثين',\n",
              " 'اربعين',\n",
              " 'خمسين',\n",
              " 'ستين',\n",
              " 'سبعين',\n",
              " 'ثمانين',\n",
              " 'تسعين',\n",
              " 'بضع',\n",
              " 'نيف',\n",
              " 'أجمع',\n",
              " 'جميع',\n",
              " 'عامة',\n",
              " 'عين',\n",
              " 'نفس',\n",
              " 'لا سيما',\n",
              " 'أصلا',\n",
              " 'أهلا',\n",
              " 'أيضا',\n",
              " 'بؤسا',\n",
              " 'بعدا',\n",
              " 'بغتة',\n",
              " 'تعسا',\n",
              " 'حقا',\n",
              " 'حمدا',\n",
              " 'خلافا',\n",
              " 'خاصة',\n",
              " 'دواليك',\n",
              " 'سحقا',\n",
              " 'سرا',\n",
              " 'سمعا',\n",
              " 'صبرا',\n",
              " 'صدقا',\n",
              " 'صراحة',\n",
              " 'طرا',\n",
              " 'عجبا',\n",
              " 'عيانا',\n",
              " 'غالبا',\n",
              " 'فرادى',\n",
              " 'فضلا',\n",
              " 'قاطبة',\n",
              " 'كثيرا',\n",
              " 'لبيك',\n",
              " 'معاذ',\n",
              " 'أبدا',\n",
              " 'إزاء',\n",
              " 'أصلا',\n",
              " 'الآن',\n",
              " 'أمد',\n",
              " 'أمس',\n",
              " 'آنفا',\n",
              " 'آناء',\n",
              " 'أنّى',\n",
              " 'أول',\n",
              " 'أيّان',\n",
              " 'تارة',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'حقا',\n",
              " 'صباح',\n",
              " 'مساء',\n",
              " 'ضحوة',\n",
              " 'عوض',\n",
              " 'غدا',\n",
              " 'غداة',\n",
              " 'قطّ',\n",
              " 'كلّما',\n",
              " 'لدن',\n",
              " 'لمّا',\n",
              " 'مرّة',\n",
              " 'قبل',\n",
              " 'خلف',\n",
              " 'أمام',\n",
              " 'فوق',\n",
              " 'تحت',\n",
              " 'يمين',\n",
              " 'شمال',\n",
              " 'ارتدّ',\n",
              " 'استحال',\n",
              " 'أصبح',\n",
              " 'أضحى',\n",
              " 'آض',\n",
              " 'أمسى',\n",
              " 'انقلب',\n",
              " 'بات',\n",
              " 'تبدّل',\n",
              " 'تحوّل',\n",
              " 'حار',\n",
              " 'رجع',\n",
              " 'راح',\n",
              " 'صار',\n",
              " 'ظلّ',\n",
              " 'عاد',\n",
              " 'غدا',\n",
              " 'كان',\n",
              " 'ما انفك',\n",
              " 'ما برح',\n",
              " 'مادام',\n",
              " 'مازال',\n",
              " 'مافتئ',\n",
              " 'ابتدأ',\n",
              " 'أخذ',\n",
              " 'اخلولق',\n",
              " 'أقبل',\n",
              " 'انبرى',\n",
              " 'أنشأ',\n",
              " 'أوشك',\n",
              " 'جعل',\n",
              " 'حرى',\n",
              " 'شرع',\n",
              " 'طفق',\n",
              " 'علق',\n",
              " 'قام',\n",
              " 'كرب',\n",
              " 'كاد',\n",
              " 'هبّ']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n"
      ],
      "metadata": {
        "id": "xhkYARdxdZJy"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "xO8INyf1ds3b"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "WTv9FNMHdv9x"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwRRQsV4dyVH",
        "outputId": "ba86678a-b5dd-4428-c33d-19ded7ead8f5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "J-mcRcBLd_yO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYDPgehjeIXg",
        "outputId": "6304d752-7cb5-4ec9-b114-86028edfbeb1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['onc , live king .',\n",
              " 'the king rich .',\n",
              " 'the king serv mani peopl kingdom .',\n",
              " 'he richest man kingdom .']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WZiR2mbeKKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}